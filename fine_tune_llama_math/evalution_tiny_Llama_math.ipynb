{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc979ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "adapter_path = './tinyllama-lora-tuned-adapter-math'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ").eval()\n",
    "\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "tuned_model = tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['question'], batch['answer'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset('openai/gsm8k', 'main', split='train[:20]')\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d89b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ec9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model):\n",
    "    losses = []\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return math.exp(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea122c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "raw_data = load_dataset('openai/gsm8k' , 'main' , split='train[:200]')\n",
    "refs = raw_data['answer']\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25621105",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f055cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(base_model, raw_data['question'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(tuned_model, raw_data['question'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa451056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a848223",
   "metadata": {},
   "source": [
    "UNSEEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset('openai/gsm8k', 'main', split='train[200:300]')\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191125d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e446b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_dataset('openai/gsm8k', 'main', split='train[200:300]')\n",
    "refs = raw_data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_dataset('openai/gsm8k', 'main', split='train[200:300]')\n",
    "refs = raw_data['answer']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(base_model, raw_data['question'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
